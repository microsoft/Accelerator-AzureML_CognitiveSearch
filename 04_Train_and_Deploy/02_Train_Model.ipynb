{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Connect to Workspace\n",
    "Initialize a Workspace object from the existing workspace you created in the Prerequisites step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "    print(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default datastore object\n",
    "ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from tempfile import TemporaryDirectory\n",
    "from azureml.core import Dataset, Run\n",
    "import pandas as pd\n",
    "import torch\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "from utils_nlp.common.pytorch_utils import dataloader_from_dataset\n",
    "from utils_nlp.common.timer import Timer\n",
    "from utils_nlp.dataset.ner_utils import preprocess_conll\n",
    "from utils_nlp.models.transformers.named_entity_recognition import (\n",
    "    TokenClassificationProcessor, TokenClassifier)\n",
    "\n",
    "\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "\n",
    "# the data path used to save the downloaded data file\n",
    "DATA_PATH = TemporaryDirectory().name\n",
    "# the cache data path during find tuning\n",
    "CACHE_DIR = TemporaryDirectory().name\n",
    "# set random seeds\n",
    "RANDOM_SEED = 100\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "# model configurations\n",
    "model_name = \"bert-base-cased\"\n",
    "DO_LOWER_CASE = False\n",
    "TRAILING_PIECE_TAG = \"X\"\n",
    "DEVICE = \"cuda\"\n",
    "max_len = 256\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"get data\"\"\"\n",
    "run = Run.get_context()\n",
    "workspace = run.experiment.workspace\n",
    "dataset_name = 'ner_ds_file'\n",
    "# Get a dataset by name\n",
    "file_ds = Dataset.get_by_name(workspace=workspace, name=dataset_name)\n",
    "file_downloads=file_ds.download()\n",
    "\n",
    "# preprocess conll format\n",
    "with open(file_downloads[0], \"r\", encoding=\"utf8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "sentence_list, labels_list = preprocess_conll(text)\n",
    "\n",
    "processor = TokenClassificationProcessor(model_name=model_name, to_lower=DO_LOWER_CASE, cache_dir=CACHE_DIR)\n",
    "\n",
    "label_map = TokenClassificationProcessor.create_label_map(\n",
    "    label_lists=labels_list, trailing_piece_tag=TRAILING_PIECE_TAG\n",
    ")\n",
    "\n",
    "train_dataset = processor.preprocess_for_bert(\n",
    "    text=sentence_list,\n",
    "    max_len=max_len,\n",
    "    labels=labels_list,\n",
    "    label_map=label_map,\n",
    "    trailing_piece_tag=TRAILING_PIECE_TAG,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = dataloader_from_dataset(\n",
    "    train_dataset, batch_size=BATCH_SIZE, num_gpus=None, shuffle=True, distributed=False\n",
    ")\n",
    "\n",
    "\n",
    "# Instantiate a TokenClassifier class for NER using pretrained transformer model\n",
    "model = TokenClassifier(\n",
    "    model_name=model_name,\n",
    "    num_labels=len(label_map),\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "# Fine tune the model using the training dataset\n",
    "with Timer() as t:\n",
    "    model.fit(\n",
    "        train_dataloader=train_dataloader,\n",
    "        num_epochs=NUM_TRAIN_EPOCHS,\n",
    "        num_gpus=None,\n",
    "        local_rank=-1,\n",
    "        weight_decay=0.0,\n",
    "        learning_rate=5e-5,\n",
    "        adam_epsilon=1e-8,\n",
    "        warmup_steps=0,\n",
    "        verbose=True,\n",
    "        seed=RANDOM_SEED,\n",
    "    )\n",
    "\n",
    "\n",
    "#save\n",
    "torch.save(model.model.state_dict(), 'nlprecipes_bert_ner.model')\n",
    "\n",
    "# get hold of the current run\n",
    "run.upload_file(\"outputs/nlprecipes_bert_ner.model\", \"nlprecipes_bert_ner.model\")\n",
    "\n",
    "with open(DATA_PATH, \"w\") as f:\n",
    "        f.write(json.dumps(label_map))\n",
    "\n",
    "# get hold of the current run\n",
    "#Save the label map as json file and load it as dictionary in score script\n",
    "run.upload_file(\"outputs/labelfile.txt\", DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Create and Attach Compute for model training\n",
    "There are two compute options: run once (preview) and persistent computer for this demo we will use persistent compute to learn more about run once compute check out the docs. If VM size STANDARD_NC12S_V2 is not be available in your subscription, use Standard_NC12s_v2 or similar instead.\n",
    "\n",
    "### *Important*\n",
    "Run-based creation of Azure Machine Learning compute is currently in Preview. Don't use run-based creation if you use automated hyperparameter tuning or automated machine learning. To use hyperparameter tuning or automated machine learning, create a persistent compute target instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cluster_name = \"kmgpu-cluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NV12s_v2',\n",
    "                                                           min_nodes=1,\n",
    "                                                           max_nodes=4)\n",
    "    cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Create An Experiment\n",
    "\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment/?WT.mc_id=bert-notebook-abornst) to track all the runs in your workspace for this distributed PyTorch tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'bertkmnlp'\n",
    "\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remote env config\n",
    "PIP_PACKAGES = [\"seqeval[gpu]\", \"torch==1.4\", \"tqdm==4.31.1\", \"transformers==2.8.0\", \"nltk==3.5\", \"azureml-sdk==1.3.0\"]\n",
    "CONDA_PACKAGES = [\"numpy\", \"scikit-learn\", \"pandas\"]\n",
    "utils_nlp_file=\"./nlp-recipes-utils/utils_nlp-2.0.0-py3-none-any.whl\"\n",
    "PYTHON_VERSION = \"3.6.8\"\n",
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda env setup\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.environment import Environment, DEFAULT_GPU_IMAGE\n",
    "\n",
    "myenv = Environment(name=\"myenv\")\n",
    "\n",
    "conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=CONDA_PACKAGES,\n",
    "    pip_packages=PIP_PACKAGES,\n",
    "    python_version=PYTHON_VERSION,\n",
    ")\n",
    "\n",
    "nlp_repo_whl = Environment.add_private_pip_wheel(\n",
    "    workspace=ws,\n",
    "    file_path=utils_nlp_file,\n",
    "    exist_ok=True,\n",
    ")\n",
    "#we can also add using the approach mentioned at https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments#add-packages-to-an-environment\n",
    "\n",
    "conda_dependencies.add_pip_package(nlp_repo_whl)\n",
    "\n",
    "# Adds dependencies to PythonSection of myenv\n",
    "myenv.python.conda_dependencies=conda_dependencies\n",
    "\n",
    "\n",
    "\n",
    "# Add training script to run config\n",
    "runconfig = ScriptRunConfig(source_directory=\".\", script=\"train.py\" )\n",
    "# Attach compute target to run config\n",
    "runconfig.run_config.target = cluster\n",
    "\n",
    "# Attach environment to run config\n",
    "runconfig.run_config.environment = myenv\n",
    "runconfig.run_config.environment.docker.enabled = True\n",
    "\n",
    "if USE_GPU:\n",
    "    runconfig.run_config.environment.docker.base_image = DEFAULT_GPU_IMAGE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit run \n",
    "run = exp.submit(runconfig)\n",
    "run.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Shows output of the run on stdout.\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this experiment typically takes 1-2 hours to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Register the Model\n",
    "Register the model \"bertkm_ner\" that was created in the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incase you lost access to the notebook when this model was running for long time\n",
    "# from azureml.core import Experiment, Run\n",
    "# experiment_name = 'bertkmnlp'\n",
    "\n",
    "# exp = Experiment(workspace=ws, name=experiment_name)\n",
    "# run=Run(exp, \"bertkmnlp_XXXXXXXXXXXXXXX\", outputs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(model_name='bertkm_ner', model_path='outputs/nlprecipes_bert_ner.model')\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Label Map file which will be used as part of Inferencing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file(\"outputs/labelfile.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "Next we need to deploy the model as a web service. Follow the steps in 03_Deploy_to_AKS.ipynb to deploy the model to  AKS.\n",
    "\n",
    "Note: You can debug Score script locally. Follow the steps mentioned in 04_Debug_Score_Script.ipynb to develop and debug score script locally. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}